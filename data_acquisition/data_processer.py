"""
Given a CSV generated by the Web Scraper IO tool, process the data for our use.
"""
import argparse
import csv
import json
import re
import sys
from io import TextIOWrapper
from typing import Any, Callable, Dict, List, Optional, Text, TextIO

# This field_size_limit is needed to import the data CSV
max_int = sys.maxsize
while True:
  try:
    csv.field_size_limit(max_int)
    break
  except OverflowError:
    max_int = int(max_int/10)


class EntryParser():
  def __init__(self,
    output_field: Text,
    input_field: Text,
    parser: Callable[[Any], Text]
  ) -> None:
    self.output_field: Text = output_field
    self.input_field: Text = input_field
    self.parser: Callable[[Text], Text] = parser
  
  def parse(self, x: Text) -> Text:
    if (not x) or (x == "null"):
      raise ValueError("Input string is empty.")
    return self.parser(x)


# Start: parser function implementations
# TODO: Maybe move this into a different file. Kept this here for development.
def parser_compress_all_whitespace(x: Text) -> Text:
  return re.sub(r"\s+", " ", x)


def parser_compress_whitespace_from_json(json_key: Text) -> Callable:

  def inner_function(json_str: Text) -> Text:
    json_array = json.loads(json_str)
    if not json_array:
      raise ValueError("JSON string is not readable.")

    text_array = [
      parser_compress_all_whitespace(item[json_key])
      for item in json_array
    ]
    return "\n".join(
      item for item in text_array
      if item
    )

  return inner_function


def parser_get_year_from_string(x: Text) -> Text:
  match: Optional[re.Match] = re.search("\d{4}", x)
  if match:
    return match.group()
  else:
    raise ValueError(
      "Could not find a year in the following string: {}".format(x)
    )

# End: parser function implementations

def process_data(
  input_file_loc: Text,
  output_file_loc: Text,
) -> None:
  print("Input csv location: {}".format(input_file_loc))
  print("Output csv location: {}".format(output_file_loc))

  """
  Original csv headers:
    web-scraper-order,
    web-scraper-start-url,
    speech_list_page,
    speech_list_page-href,
    speech_page,
    speech_page-href,
    speech-speech_name,
    speech-subtitle,
    speech-speaker_name,
    speech-raw_html,
    speech-transcript_json
  """
  
  # TODO: try to parse date from the subtitle (maybe just the year)
  output_csv_parsers: List[EntryParser] = [
    EntryParser(
      output_field="title",
      input_field="speech-speech_name",
      parser=parser_compress_all_whitespace
    ),
    EntryParser(
      output_field="speaker",
      input_field="speech-speaker_name",
      parser=parser_compress_all_whitespace
    ),
    EntryParser(
      output_field="transcript",
      input_field="speech-transcript_json",
      parser=parser_compress_whitespace_from_json("speech-transcript_json")
    ),

    # Extra metadata (experimental)
    # EntryParser(
    #   output_field="subtitle",
    #   input_field="speech-subtitle",
    #   parser=parser_compress_all_whitespace
    # ),
    EntryParser(
      output_field="year",
      input_field="speech-subtitle",
      parser=parser_get_year_from_string
    )
  ]

  output_csv_fields: List[Text] = [p.output_field for p in output_csv_parsers]

  # output_csv_fields = [
  #   "title",
  #   "speaker",
  #   "transcript",
  #   # Extra metadata (experimental)
  #   "subtitle",
  #   "year"
  # ]

  with open(input_file_loc, "r", newline="", encoding="utf-8-sig") as input_file:
    input_reader = csv.DictReader(
      input_file, quoting=csv.QUOTE_ALL
    )

    with open(output_file_loc, "w", newline="", encoding="utf-8") as output_file:
      output_writer = csv.DictWriter(
        output_file, quoting=csv.QUOTE_ALL, fieldnames=output_csv_fields
      )
      output_writer.writeheader()

      num_input_entries: int = 0
      num_output_entries: int = 0

      for entry in input_reader:
        num_input_entries += 1

        # Processing each piece of data, as needed
        try:
          output_dict: Dict[Text, Text] = {
            entry_parser.output_field : entry_parser.parse(
              entry[entry_parser.input_field]
            )
            for entry_parser in output_csv_parsers
          }

          try:
            output_writer.writerow(output_dict)
          except Exception as e:
            raise ValueError("Error writing following dict: {}\n{}".format(
              output_dict, e
            ))

          num_output_entries += 1

        except Exception as e:
          print("Error encountered, skipping:\n\tSpeech: {}\n\tSpeaker: {}\n\tLink: {}\n\tError: {}\n".format(
            parser_compress_all_whitespace(entry["speech-speech_name"]),
            parser_compress_all_whitespace(entry["speech-speaker_name"]),
            entry["speech_page-href"],
            e
          ))
      
      print("\nNumber of input entries: {}".format(num_input_entries))
      print("Number of output entries: {}".format(num_output_entries))


def parse_args() -> argparse.Namespace:
  parser: argparse.ArgumentParser = argparse.ArgumentParser(description="Process web scraped data.")
  parser.add_argument(
    "-i", "--input", type=str, required=True,
    help="Input csv location"
  )

  parser.add_argument(
    "-o", "--output", type=str, required=True,
    help="Location (csv) where to output processed data"
  )

  parser.add_argument(
    "-l", "--log", type=str, required=False,
    help="(Optional) Location where to output .txt log file"
  )

  return parser.parse_args()


class Logger(TextIOWrapper):
  def __init__(self, log_file, stream) -> None:
    self.__stream = stream
    self.__log_file = log_file

  def write(self, x) -> int:
    self.__log_file.write(x)
    self.__stream.write(x)
    self.__log_file.flush()
    self.__stream.flush()

    return len(x)


class LoggerFactory():
  def __init__(self, log_file_loc) -> None:
    self.__log_file: Optional[TextIO]
    if log_file_loc:
      self.__log_file = open(log_file_loc, "w", encoding="utf-8")
    else:
      self.__log_file = None

  def __enter__(self) -> "LoggerFactory":
    return self
  
  def __exit__(self, exc_type, exc_value, traceback) -> None:
    if self.__log_file:
      self.__log_file.close()
  
  def get_logger(self, stream) -> Logger:
    if self.__log_file:
      return Logger(self.__log_file, stream)
    else:
      return stream

  def set_loggers(self) -> None:
    sys.stdout = self.get_logger(sys.stdout)
    sys.stderr = self.get_logger(sys.stderr)


def main() -> None:
  args: argparse.Namespace = parse_args()
  
  log_file_loc = args.log if args.log else ""
  with LoggerFactory(log_file_loc) as logger_factory:
    logger_factory.set_loggers()

    process_data(
      input_file_loc = args.input,
      output_file_loc = args.output
    )


if __name__ == "__main__":
  main()
